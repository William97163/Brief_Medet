{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp81GSFN7uHY"
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "**Exercise objetives**\n",
    "- Use a pretrained neural network : Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAkNOgtqF7S_"
   },
   "source": [
    "#### Transfer what?\n",
    "We have seen that the convolutions are mathematical operation that detect specific patterns in input images and use them to classify the image. One could imagine that these patterns are not 100% specific to the task but to the input images. \n",
    "\n",
    "üí° Therefore, **why not using convolutions that have been learnt on other task** with the expectation that it will also work in other scenario?  We _transfer_ a CNN from one task to another => _transfer learning_. This has two advantages:\n",
    "- taking less time to train\n",
    "- benefiting from complex architecture that have been trained for state-of-the-art challenges. \n",
    "\n",
    "‚ö†Ô∏è Although convolutions may not be specific, the last layer is by design specific to the problem it was trained on! Therefore, this last layer is usually removed, replace by a layer that is design to the task. As this new last layer has random weight, it has to be retrained. This is called _fine-tunning_. \n",
    "\n",
    "#### VGG16\n",
    "In this exercise, we will use the [VGG-16 Neural Network](https://neurohive.io/en/popular-networks/vgg16/), a well-known architecture that has been trained on ImageNet which is a very large database of images of different categories. In a nutshell, this architecture has already learnt kernels which are supposed to be good not only for the task it has been train on but maybe for other tasks. \n",
    "\n",
    "<center><img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" width=400></center>\n",
    "\n",
    "The idea is that first layers are not specialized for the particular task it has been trained on ; only the last ones are. Therefore, we will \n",
    "- load the existing VGG16 network\n",
    "- remove the last fully connected layers\n",
    "- replace them by new connected layers (whose weights are randomly set)\n",
    "- and train these last layers on a specific classification task - here, separate types of flower. \n",
    "\n",
    "The underlying idea is that the first convolutional layers of VGG-16, that has already been trained, corresponds to filters that are able to extract meaning features from images. And you will only learn the last layers for your particular problem.\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qnIYWC61mCJ"
   },
   "source": [
    "* ü§Ø Tech companies and university labs have more computational resources than we do\n",
    "* üòé Let them train their super complex models on millions of images, and then re-use their kernels for our own CNNs!\n",
    "\n",
    "üéØ **<u>Goal:</u>**\n",
    "* ‚òÑÔ∏è Use a **Pretrained Neural Network** $ \\Leftrightarrow $ **Transfer learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P68gjRYG1mCK"
   },
   "source": [
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xegm30q1mCK"
   },
   "source": [
    "Repeat the same process from the last challenge to upload your challenge folder and open your notebook:\n",
    "\n",
    "1. access your [Google Drive](https://drive.google.com/)\n",
    "2. go into the Colab Notebooks folder\n",
    "3. drag and drop this challenge's folder into it\n",
    "4. right-click the notebook file and select `Open with` $\\rightarrow$ `Google Colaboratory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgPcTUZu1mCK"
   },
   "source": [
    "Don' t forget to enable GPU acceleration!\n",
    "\n",
    "`Runtime` $\\rightarrow$ `Change runtime type` $\\rightarrow$ `Hardware accelerator` $\\rightarrow$ `GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-S858KRF7TA"
   },
   "source": [
    "# 1. Data loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdS5hErJF7TB"
   },
   "source": [
    "Adding the data to Google Drive.**\n",
    "\n",
    "You can first download the data   `flowers-dataset`. Then you have to add them to your Google Drive in a folder called `Deep_learning_data` (for instance) and run the following code in the notebook.: \n",
    "\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "```\n",
    "\n",
    "The previous code will ask you to go to a given webpage where you copy the link and past it in the Colab form that will appear. Do so to load the data on Google Colab.\n",
    "\n",
    "\n",
    "\n",
    "‚ùì **Question** ‚ùì  load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T18:59:38.944033Z",
     "start_time": "2021-04-27T18:56:51.915983Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKw_1TjOF7TC",
    "outputId": "6774b5b0-4407-4aef-bae8-131cfc5f2af4"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUF41j3D1mCM",
    "outputId": "318f15f5-6ebe-4270-db85-d97707226e8f"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJcLaz_m1mCM",
    "outputId": "5d6fe437-2a35-4f2f-f9b4-200824c17bd0"
   },
   "outputs": [],
   "source": [
    " !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yt175eok5DXv",
    "outputId": "732f7c29-37f7-42da-c6cf-31e67d96942b"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow.keras.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onPwIzQzF7TE"
   },
   "source": [
    "‚ùì **Question** ‚ùì Use the following method to create \n",
    "`X_train, y_train, X_val, y_val, X_test, y_test, num_classes` depending on the `loading_method` you have used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T20:46:49.729732Z",
     "start_time": "2021-04-27T20:46:49.715480Z"
    },
    "id": "XPJiWZVjF7TF"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_flowers_data(loading_method):\n",
    "    if loading_method == 'colab':\n",
    "        data_path = '/content/drive/MyDrive/Deep_learning_data/flowers/'\n",
    "    elif loading_method == 'direct':\n",
    "        data_path = 'flowers-dataset/flowers/'\n",
    "    classes = {'daisy':0, 'dandelion':1, 'rose':2}\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for (cl, i) in classes.items():\n",
    "        images_path = [elt for elt in os.listdir(os.path.join(data_path, cl)) if elt.find('.jpg')>0]\n",
    "        for img in tqdm(images_path[:300]):\n",
    "            path = os.path.join(data_path, cl, img)\n",
    "            if os.path.exists(path):\n",
    "                image = Image.open(path)\n",
    "                image = image.resize((256, 256))\n",
    "                imgs.append(np.array(image))\n",
    "                labels.append(i)\n",
    "\n",
    "    X = np.array(imgs)\n",
    "    num_classes = len(set(labels))\n",
    "    y = to_categorical(labels, num_classes)\n",
    "\n",
    "    # Finally we shuffle:\n",
    "    p = np.random.permutation(len(X))\n",
    "    X, y = X[p], y[p]\n",
    "\n",
    "    first_split = int(len(imgs) /6.)\n",
    "    second_split = first_split + int(len(imgs) * 0.2)\n",
    "    X_test, X_val, X_train = X[:first_split], X[first_split:second_split], X[second_split:]\n",
    "    y_test, y_val, y_train = y[:first_split], y[first_split:second_split], y[second_split:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T20:46:55.110063Z",
     "start_time": "2021-04-27T20:46:50.487722Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlO39-AMF7TG",
    "outputId": "ad778adc-3492-4c57-a32a-97d707e467d5"
   },
   "outputs": [],
   "source": [
    "# CALL load_flowers_data \n",
    "\n",
    "datas = load_flowers_data('colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm2IsMmGF7TH"
   },
   "source": [
    "‚ùì Check image shape and plot few of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "id": "BRrO7DVEO3uV",
    "outputId": "d0fc37c0-e00b-45c3-aeb7-91ebab1a9389"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the flowers data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, num_classes = load_flowers_data('colab')\n",
    "\n",
    "# Check the shape of the images\n",
    "print(f\"Training images shape: {X_train.shape}\")\n",
    "print(f\"Validation images shape: {X_val.shape}\")\n",
    "print(f\"Test images shape: {X_test.shape}\")\n",
    "\n",
    "# Plot a few images\n",
    "fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(9):\n",
    "    axs[i].imshow(X_train[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxMhrBQ21mCN"
   },
   "source": [
    "## (1) What is a Pre-Trained Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HrKIvTO1mCO"
   },
   "source": [
    "* Convolutions are mathematical operations designed to detect specific patterns in input images and use them to classify the images. \n",
    "* One could imagine that these patterns are not 100% specific to one task but to the input images. \n",
    "\n",
    "üöÄ **Why not re-use these kernels - whose weights have already been optimized - somewhere else?** \n",
    "- The expectation is that the trained kernels could also help us perform another classification task.\n",
    "- We are trying to ***transfer*** the knowledge of a trained CNN to a new classification task.\n",
    "\n",
    "\n",
    "üí™ Transfer Learning has two main advantages:\n",
    "- It takes less time to train a pre-trained model since we are not going to update all the weights but only some of them\n",
    "- You benefit from state-of-the-art architectures that have been trained on complex images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFdyaz231mCO"
   },
   "source": [
    "## (2) Introduction to  VGG16 \n",
    "\n",
    "üìö ***Reading Section, no code***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUxAOD1B1mCO"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this exercise, we will use the <a href=\"https://neurohive.io/en/popular-networks/vgg16/\">**`VGG-16 Neural Network`**</a>.\n",
    "\n",
    "> VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper ‚ÄúVery Deep Convolutional Networks for Large-Scale Image Recognition‚Äù. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3√ó3 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU‚Äôs.\n",
    "\n",
    "VGG16 is a well-known architecture that has been trained on the <a href=\"https://www.image-net.org/\">**`ImageNet dataset`**</a> which is a very large database of images which belong to different categories. \n",
    "\n",
    "üëâ This architecture already learned which kernels are the best for extracting features from the images found in the `ImageNet dataset`.\n",
    "\n",
    "üëâ As you can see in the illustration, the VGG16 involves millions of parameters you don't want to retrain yourself.\n",
    "\n",
    "\n",
    "<center><img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" width=400></center>\n",
    "\n",
    "‚ùì How does it work in practice ‚ùì\n",
    "\n",
    "* The first layers are not specialized for the particular task the VGG16 CNN was trained on\n",
    "* Only the last dense layer is a \"classification layers\" that can be preceded with a couple of dense layers...  Therefore, we will: \n",
    "    1. Load the existing VGG16 network\n",
    "    2. Remove the last fully connected layers\n",
    "    3. Replace them with some new fully-connected layers (whose weights are randomly set)\n",
    "    4. Train these last layers on a specific classification task. \n",
    "\n",
    "üòÉ Your role is to train only the last layers for your particular problem.\n",
    "\n",
    "ü§ì We will use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16\">**`tensorflow.keras.applications.VGG16`**</a>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdMfP6dyF7TJ"
   },
   "source": [
    "# 2. Home made model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njYcWjw1F7TJ"
   },
   "source": [
    "First, let's check our performance on a home-made CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T18:59:47.855534Z",
     "start_time": "2021-04-27T18:59:38.947172Z"
    },
    "id": "wneAeLYdF7TK"
   },
   "source": [
    "‚ùì **Questions** ‚ùì \n",
    "\n",
    "- Build, compile and fit a CNN model adapted to the challenge.\n",
    "- Compare performance with baseline \n",
    "- We recommand to use the following architecture\n",
    "\n",
    "---\n",
    "```python\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Notice this cool new layer that \"pipe\" your rescaling within the architecture\n",
    "model.add(Rescaling(1./255, input_shape=(256,256,3)))\n",
    "\n",
    "# Lets add 3 convolution layers, with relatively large kernel size as our pictures are quite big too\n",
    "model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQupI--81mCP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Notice this cool new layer that \"pipe\" your rescaling within the architecture\n",
    "model.add(Rescaling(1./255, input_shape=(256,256,3)))\n",
    "\n",
    "# Lets add 3 convolution layers, with relatively large kernel size as our pictures are quite big too\n",
    "model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB1gu7WD1mCP"
   },
   "source": [
    "ü•° <b><u>Takeaways from building your own CNN</u></b>:\n",
    "* On an \"easy dataset\" like the MNIST, it is now easy to reach a decent accuracy. But for a more complicated problem like classifying flowers, it already becomes more challenging. Take a few minutes to play with the following link before moving on to Transfer Learning\n",
    "    * [PoloClub/CNN-Explainer](https://poloclub.github.io/cnn-explainer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx8V9sny7cLP"
   },
   "source": [
    "# 3. Transfer learning = Pre-entrained CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxsL77ft1mCP"
   },
   "source": [
    "As we said in the beginning, tech companies and university labs have more computational resources than we do.\n",
    "\n",
    "üî• The [**Visual Geometry Group**](https://www.robots.ox.ac.uk/~vgg/data/) *(Oxford University, Department of Science and Engineering)* became famous for some of their **Very Deep Convolutional Neural Networks**: the [**VGG16**](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "\n",
    "Take 7 minutes of your time to watch this incredible video of Convolutional Layers created by Dimitri Dmitriev.\n",
    "\n",
    "* üì∫ **[VGG16 Neural Network Visualization](https://www.youtube.com/watch?v=RNnKtNrsrmg)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr6m5eKs9s54"
   },
   "source": [
    "## 3.1 Load VGG16 model\n",
    "\n",
    "\n",
    "‚ùì **Question** ‚ùì Write a first function `load_model()` that loads the pretrained VGG-16 model from `tensorflow.keras.applications.vgg16`. Especially, look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16) to load the model where:\n",
    "- the `weights` have been learnt on `imagenet`\n",
    "- the `input_shape` corresponds to the input shape of any of your images - you have to resize them in case they are not of the same size\n",
    "- the `include_top` argument is set to `False` in order not to load the fully-connected layers of the VGG-16 without the last layer which was specifically trained on `imagenet`\n",
    "\n",
    "‚ùó **Remark** ‚ùó Do not change the default value of the other arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T14:10:03.658988Z",
     "start_time": "2021-04-27T14:10:03.653838Z"
    },
    "id": "fQAVMEWLF7TN"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "def load_model():\n",
    "    model = VGG16(weights='imagenet', input_shape=(256,256,3), include_top=False)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3psG3JGF7TO"
   },
   "source": [
    "‚ùì **Question** ‚ùì Look at the architecture of the model thanks to the summary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T14:10:06.490423Z",
     "start_time": "2021-04-27T14:10:05.969604Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtBpKLegF7TO",
    "outputId": "3cf8e6f2-e186-459d-d0d3-cc85226bf7ec",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "load_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1IAwxRVF7TO"
   },
   "source": [
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\">\n",
    "\n",
    "Impressive, right? Two things to notice:\n",
    "- It ends with a convolution layer (namely a maxpooling layer that is the layer that follows a convolution). The flattening of the output and the fully connected layers are not here yet! We need to add them !\n",
    "- There are more than 14.000.000 parameters, which is a lot. We could fine-tune them, meaning update them as we will update the last layers weights, but it will take a lot of time. For that reason, we will inform the model that the layers until the flattening are non-trainable.\n",
    "\n",
    "‚ùì **Question** ‚ùì Write a first function that takes the previous model as input the set the first layers to be non-trainable, by applying `model.trainable = False`. Then check-out the summary of the model to see that now, the parameters are `non-trainable`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bIXwQWZ1mCQ"
   },
   "outputs": [],
   "source": [
    "def set_nontrainable_layers(model):\n",
    "\n",
    "    # Set the first layers to be untrainable\n",
    "    model.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4yedT2VF7TP"
   },
   "source": [
    "‚ùì **Question** ‚ùì We will write a function that adds flattening and dense layers after the first convolutional layers. To do so, cannot directly use the classic `layers.Sequential()` instantiation.\n",
    "\n",
    "For that reason, we will see another one here. The idea is that we define each layer (or group of layers) separately. Then, we concatenate them. See this example : \n",
    "\n",
    "---\n",
    "```python\n",
    "base_model = load_model()\n",
    "base_model = set_nontrainable_layers(base_model)\n",
    "flattening_layer = layers.Flatten()\n",
    "dense_layer = layers.Dense(SOME_NUMBER_1, activation='relu')\n",
    "prediction_layer = layers.Dense(SOME_NUMBER_2, activation='APPROPRIATE_ACTIVATION')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  flattening_layer,\n",
    "  dense_layer,\n",
    "  prediction_layer\n",
    "])\n",
    "\n",
    "```\n",
    "---\n",
    "The first line loads a group of layer which is the previous VGG-16 model. Then, we set this layers to be non-tranable. Then, we can instantiate as many layers as we want.\n",
    "\n",
    "Finally, we use the `Sequential` with the sequence of layers that will correspond to our overall neural network. \n",
    "\n",
    "Replicate the following steps by adding a flattening and two dense layers (the first with 500 neurons) to the previous VGG-16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfgJDh_9F7TQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "def add_last_layers(model):\n",
    "    '''Take a pre-trained model, set its parameters as non-trainables, and add additional trainable layers on top'''\n",
    "    base_model = set_nontrainable_layers(model)\n",
    "    flattening_layer = layers.Flatten()\n",
    "    dense_layer = layers.Dense(500, activation='relu')\n",
    "    prediction_layer = layers.Dense(3, activation='softmax')\n",
    "\n",
    "    model = Sequential([\n",
    "    base_model,\n",
    "    flattening_layer,\n",
    "    dense_layer,\n",
    "    prediction_layer\n",
    "  ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-24j0psF7TQ"
   },
   "source": [
    "‚ùì **Question** ‚ùì Now look at the layers and parameters of your model. Note that there is a distinction, at the end, between the trainable and non-trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uqGuxsJF7TR",
    "outputId": "ce0a59b1-3fd5-4307-a09a-d28c60ba43b5",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "add_last_layers(load_model()).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxXlnPp5F7TR"
   },
   "source": [
    "‚ùì **Question** ‚ùì Write a function that build (and compile) your model - we advise Adam with `learning_rate=1e-4`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kscw585CF7TS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model():\n",
    "    model = add_last_layers(load_model())\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= Adam(learning_rate=0.0004))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbkwOw1eF7TS"
   },
   "source": [
    "## 3.2 Back to the data\n",
    "\n",
    "The VGG16 model was trained on images which were preprocessed in a specific way. This is the reason why we did not normalized them earlier.\n",
    "\n",
    "‚ùì **Question** ‚ùì Apply this processing to the original (non-normalized) images here using the method `preprocess_input` that you can import from `tensorflow.keras.applications.vgg16`. See [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B--Gyb-23YDb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNeJZvtV3YDf",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train_p = preprocess_input(X_train, data_format=None)\n",
    "X_val_p = preprocess_input(X_val, data_format=None)\n",
    "X_test_p = preprocess_input(X_test, data_format=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu2H0KZF-EoI"
   },
   "source": [
    "## 3.3 Fit the model\n",
    "\n",
    "‚ùì **Question** ‚ùì Now estimate the model, with an early stopping criterion on the validation accuracy - here, the validation data are provided, therefore use `validation_data` instead of `validation_split`.\n",
    "\n",
    "‚ùó **Remark** ‚ùó Store the results in a `history` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grmnNmjeAXcQ",
    "outputId": "6d830fc5-0f2d-443e-f903-8fa76358fa22",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = load_model()\n",
    "model = build_model()\n",
    "\n",
    "callback = EarlyStopping(\n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(X_train_p, y_train, epochs=100, validation_data=(X_val_p, y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec_I9JpiAm-W"
   },
   "source": [
    "‚ùì **Question** ‚ùì Plot the accuracy for the test and validation set using the usual function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HADG7rn8F7TU"
   },
   "outputs": [],
   "source": [
    "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
    "    if axs is not None:\n",
    "        ax1, ax2 = axs\n",
    "    else:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
    "        exp_name = '_' + exp_name\n",
    "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
    "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
    "    #ax1.set_ylim(0., 2.2)\n",
    "    ax1.set_title('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
    "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
    "    #ax2.set_ylim(0.25, 1.)\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    return (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "ESzinGOY6aBc",
    "outputId": "9d388631-6e4e-4960-bd82-d19f41e433d1",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3plexlQAtcC"
   },
   "source": [
    "‚ùì **Question** ‚ùì Evaluate the model accuracy on the test set. Did we improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ps_9HwUyRVj9",
    "outputId": "349ce423-aab0-4c88-f964-ce9d97d55723",
    "scrolled": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test_p, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF39HIb7BSOy"
   },
   "source": [
    "# (OPTIONAL). Improve the model\n",
    "\n",
    "You can here try to improve the model test accuracy. To do that, here are some options you can consider\n",
    "\n",
    "1. **Unfreeze and finetune**: As per [Google tutorial](https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning) \n",
    ">_Once your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate. This is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting -- keep that in mind. It is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features. It's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to readapt the pretrained weights in an incremental way._\n",
    "\n",
    "\n",
    "1. Add **data augmentation** if your model is overfitting. \n",
    "\n",
    "1. If your model is not - unlikely here - , try a more complex model.\n",
    "\n",
    "1. Perform precise **grid search** on all the hyper-parameters: learning_rate, batch_size, data augmentation etc...\n",
    "\n",
    "1. **Change the base model** to more modern one (resnet, efficient net1. available in the keras library\n",
    "\n",
    "1. Curate the data: maintaining a sane data set is one of the keys to success.\n",
    "\n",
    "1. Obtain more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydI5gpf01mCU"
   },
   "source": [
    "## (6.1) Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TShoicSd1mCU"
   },
   "source": [
    "## üèÅ Congratulation üèÅ \n",
    "Copy this notebook from your google drive into your local repo, and commit+push your progress on github. To find where this Colab notebook has been save, click on `File --> Locate in Drive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1j2ZHYz1mCU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xp81GSFN7uHY",
    "pAkNOgtqF7S_"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
